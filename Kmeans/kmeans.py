# -*- coding: utf-8 -*-
"""Kmeans

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/199Vud6--Rd_9BWe-nzup48QyLiQ4Z8vj

#1 Clusterização
O agrupamento é uma técnica para dividir os dados em diferentes grupos, na qual os registros em cada grupo são semelhantes uns aos outros. Os grupos podem ser usados diretamente, analisando mais a fundo ou passados como uma característica ou resultado para um modelo de regressão ou classificação.

#Grupo de Consumidores
Vamos aprender a realizar um modelo de clusterização utilizando um case de segmentação de clientes de um shopping. Como podemos criar grupos de consumidores dado algumas caracteríticas de perfis?

#Sobre a base de dados:
Esse conjunto de dados ilustra alguns dados dos consumidores de um shopping. A base possui algumas features como: gênero, idade, renda anual e pontuação de gastos.
"""

import pandas as pd
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import adjusted_rand_score, silhouette_score

dados = pd.read_csv("/content/mall.csv", sep=",")
dados.head()

dados.shape

dados.isnull().sum()

dados.describe()

dados["Annual Income (k$)"].median()

dados.hist(figsize=(12,12))

plt.figure(figsize=(6,4))
sns.heatmap(dados[["Age", "Annual Income (k$)", "Spending Score (1-100)"]].corr(method="pearson"), annot=True, fmt=".1f")

dados["Gender"].value_counts()

sns.pairplot(dados, hue="Gender")
plt.show()

from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler=StandardScaler()
scaler.fit(dados[["Annual Income (k$)", "Spending Score (1-100)"]])

dados_escalonados=scaler.transform(dados[["Annual Income (k$)", "Spending Score (1-100)"]])
dados_escalonados

"""#2 - K-Means
Sobre o modelo: O K-Means parte da ideia de quebrar o espaço multidimensional de dados em partições a partir do centróide dos dados. Após inicializar os centróides de forma aleatória sobre os dados, o K-Means calcula a distância dos dados para os centros mais próximos. Esse cálculo da distância é realizado várias vezes até que os dados sejam agrupados da melhor forma possível de acordo com a distância mais próxima de um centróide (ponto centro de dado na qual será formado o grupo).

#Hiperparametros:
Definição do K. Para definir esse valor de K, é necessário utilizar o método Elbow para encontrar o melhor hiperparâmetros de K. O método Elbow consiste no cálculo da soma dos erros quadráticos.

#Vantagens:
Implementação simplificado e possui uma certa facilidade em lidar com qualquer medida de similaridade entre os dados.

#Desvantagem:
Difícil definir o melhor K. Sensível a outliers. Não consegue distinguir grupos em dados não-globulares.

Para mais informação: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

Executando o algoritmo sem feature scaling
"""

# Definindo o modelo de clusterizacao. K-MEANS com 6 clusters
kmeans = KMeans(n_clusters=6,random_state=0) #definindo os hiperparametros do algoritmo (definir o número de grupo = cluster)

#Implementando o K-Means nos dados:
kmeans.fit(dados[['Annual Income (k$)','Spending Score (1-100)']])

#Salvando os centroides de cada cluster
centroides = kmeans.cluster_centers_

#Salvando os labels dos clusters para cada exemplo
kmeans_labels = kmeans.predict(dados[['Annual Income (k$)','Spending Score (1-100)']])

# Definindo o modelo de clusterizacao. K-MEANS com 6 clusters
kmeans_escalonados = KMeans(n_clusters=6,random_state=0)

#Implementando o K-Means nos dados:
kmeans_escalonados.fit(dados_escalonados)

#Salvando os centroides de cada cluster
centroides_escalonados = kmeans_escalonados.cluster_centers_

#Salvando os labels dos clusters para cada exemplo
kmeans_labels_escalonado = kmeans_escalonados.predict(dados_escalonados)

dados_escalonados=pd.DataFrame(dados_escalonados, columns=["Annual Income (k$)", "Spending Score(1-100)"])
dados_escalonados.head()

dados_escalonados["Grupos"] = kmeans_labels_escalonado
dados_escalonados.head()

dados["Grupos"]=kmeans_labels
dados.head()

pd.Series(kmeans_labels).value_counts()

centroides #espaço tridimensional (salário e score de gasto)

"""#3.1 Clusters com feature scaling"""

# plotando os dados identificando com os seus clusters
plt.scatter(dados_escalonados[['Annual Income (k$)']], dados_escalonados[['Spending Score(1-100)']], c=kmeans_labels_escalonado, alpha=0.5, cmap='rainbow')
plt.xlabel('Salario Anual')
plt.ylabel('Pontuação de gastos')

# plotando os centroides
plt.scatter(centroides_escalonados[:, 0], centroides_escalonados[:, 1], c='black', marker='X', s=200, alpha=0.5)
plt.rcParams['figure.figsize'] = (10, 5)
plt.show()

"""#2 Clusters sem feature scaling"""

# plotando os dados identificando com os seus clusters
plt.scatter(dados[['Annual Income (k$)']],dados[['Spending Score (1-100)']], c=kmeans_labels, alpha=0.5, cmap='rainbow')
plt.xlabel('Salario Anual')
plt.ylabel('Pontuação de gastos')
# plotando os centroides
plt.scatter(centroides[:, 0], centroides[:, 1], c='black', marker='X', s=200, alpha=0.5)
plt.rcParams['figure.figsize'] = (10, 5)
plt.show()

k=list(range(1,10))
print(k)

sse=[]
for i in k:
  kmeans=KMeans(n_clusters=i, random_state=0)
  kmeans.fit(dados[["Annual Income (k$)", "Spending Score (1-100)"]])
  sse.append(kmeans.inertia_)

plt.rcParams["figure.figsize"]=(10,5)
plt.plot(k, sse, "-o")
plt.xlabel("número de cluster")
plt.ylabel("inércia")
plt.show()

dados.groupby("Grupos")["Age"].mean()

dados.groupby("Grupos")["Annual Income (k$)"]. mean()

# Definindo o modelo de clusterizacao. K-MEANS com 5 clusters
kmeans = KMeans(n_clusters=5,random_state=0)

#Implementando o K-Means nos dados:
kmeans.fit(dados[['Annual Income (k$)','Spending Score (1-100)']])

#Salvando os centroides de cada cluster
centroides = kmeans.cluster_centers_

#Salvando os labels dos clusters para cada exemplo
kmeans_labels = kmeans.predict(dados[['Annual Income (k$)','Spending Score (1-100)']])

# plotando os dados identificando com os seus clusters
plt.scatter(dados[['Annual Income (k$)']],dados[['Spending Score (1-100)']], c=kmeans_labels, alpha=0.5, cmap='rainbow')
plt.xlabel('Salario Anual')
plt.ylabel('Pontuação de gastos')
# plotando os centroides
plt.scatter(centroides[:, 0], centroides[:, 1], c='black', marker='X', s=200, alpha=0.5)
plt.rcParams['figure.figsize'] = (10, 5)
plt.show()

dados_grupo_1 = dados[dados["Grupos"] ==1]
dados_grupo_1

dados_grupo_2 = dados[dados["Grupos"] ==2]
dados_grupo_3 = dados[dados["Grupos"] ==3]
dados_grupo_4 = dados[dados["Grupos"] ==4]
dados_grupo_5 = dados[dados["Grupos"] ==5]

dados_grupo_1["Annual Income (k$)"].mean()

dados_grupo_2["Annual Income (k$)"].mean()

dados_grupo_3["Annual Income (k$)"].mean()

dados_grupo_4["Annual Income (k$)"].mean()

dados_grupo_5["Annual Income (k$)"].mean()

dados_grupo_1["Age"].mean()

dados_grupo_2["Age"].mean()

dados_grupo_3["Age"].mean()

dados_grupo_4["Age"].mean()

dados_grupo_5["Age"].mean()

dados_grupo_1["Spending Score (1-100)"].mean()

dados_grupo_2["Spending Score (1-100)"].mean()

dados_grupo_3["Spending Score (1-100)"].mean()

dados_grupo_4["Spending Score (1-100)"].mean()

dados_grupo_5["Spending Score (1-100)"].mean()

plt.figure(figsize=(6,4))
sns.heatmap(dados_grupo_1.groupby('Grupos').corr(method = 'pearson'), annot=True, fmt=".1f");

"""#3.3 - DBSCAN
Sobre o modelo: O DBSCAN é um algoritmo que agrupa os dados com base em densidade (alta concentração de dados). Muito bom para tirar ruídos. O agrupamentos dos dados é calculado com base nos core (quantidade de pontos mínmos que seja igual ou maior a definição do MinPts), border (ponto de fronteira dos dados) e noise (ruído).

#Hiperparametro:
Eps (raio ao redor de um dado). MinPts (mínimo de pontos dentro do raio para que seja agrupado).

#Vantagem:
Capacidade de trabalhar com outliers. Trabalha com base de dados grande.

#Desvantagem:
Dificuldade para lidar com cluster dentro de cluster. Dificuldade para lidar com dados de alta dimensionalidade. Dificuldade em encontrar o raio de vizinhança ao tentar agrupar dados com distância média muito distinta (clusters mais densos que outros).

Para mais informação: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html
"""

#Criando o modelo:
dbscan = DBSCAN(eps=10, min_samples=8)
#Ajustando aos dados
dbscan.fit(dados[['Annual Income (k$)','Spending Score (1-100)']])

dbscan_labels = dbscan.labels_
dbscan_labels

"""#Labels com -1 foram classificados como outliers"""

#Plotando o grafico:
plt.scatter(dados[['Annual Income (k$)']],dados[['Spending Score (1-100)']], c=dbscan_labels, alpha=0.5, cmap='rainbow')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

#Plotando o grafico sem os outliers:
#mascara para outlier
mascara = dbscan_labels>=0

#plotando o gráfico
plt.scatter(dados[['Annual Income (k$)']][mascara],dados[['Spending Score (1-100)']][mascara], c=dbscan_labels[mascara], alpha=0.5, cmap='rainbow')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

list(mascara).count(False)

"""#3.4 Como validar uma clusterização?
Temos dois tipos:

Interna: Quanto bom foi o meu agrupamento?
Externa: Como parecido estão os meus dois algoritmos comparados?
#Avaliando o Desempenho dos Algoritmos
Tipo Externo:
(a) Usando o Adjusted Rand Index

Compara o desempenho quando forem fornecidos datasets com labels geradas de forma aleatória. Quando essas labels estão muito diferente, o valor se aproxima de 0, o que sugere um resultado negativo, ou seja, clusters não próximos.

Comparação entre K-Means e DBSCAN:
"""

adjusted_rand_score(kmeans_labels,dbscan_labels)

"""#Tipo interno:
(b) Avaliando a métrica de Silhouette

Mede o formato do cluster obtido: avalia a distância entre os centros dos clusters, nesse caso, queremos maximizar as distâncias)

Valores próximos a -1, significa clusters ruins, próximo a 1, clusters bem separados.
"""

#KMEANS
silhouette_score(dados[['Annual Income (k$)','Spending Score (1-100)']],kmeans_labels)

#DBSCAN
silhouette_score(dados[['Annual Income (k$)','Spending Score (1-100)']],dbscan_labels)

"""# 4. Segmentação de Imagens de Câncer de Mama
Vamos usar o dataset mini-MIAS disponível em http://peipa.essex.ac.uk/info/mias.html.

#Este dataset possui 322 imagens de mamografias.

A Segmentação de Imagem envolve em colorir de uma mesma cor objetos ou partes de interesse. No caso do nosso dataset, iremos usar a segmentação de imagem para reduzir a resolução, aumentando o contraste entre diferentes regiões dos tecidos representados na imagem de mamografia.

O dataset é mais detalhado, possuindo inclusive tipos de câncer e anormalidades que ocorrem em cada uma das imagens.

Vamos apenas aplicar o K-Means em algumas imagens para observar o resultado:
"""

# Para processar arquivos e imagens
from PIL import Image
import glob
import numpy as np

# Para plotar imagens
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

from sklearn.cluster import KMeans # Agrupamento

# Carregando as imagens

img_G = mpimg.imread('mdb001.pgm') # Tipo G
img_D = mpimg.imread('mdb003.pgm') # Tipo D
img_F = mpimg.imread('mdb005.pgm') # Tipo F

# Plotando as imagens

fig, axs = plt.subplots(1, 3, figsize=(10, 3))
im1 = axs[0].imshow(img_G, cmap='gray', vmin=0, vmax=255)
im2 = axs[1].imshow(img_D, cmap='gray', vmin=0, vmax=255)
im3 = axs[2].imshow(img_F, cmap='gray', vmin=0, vmax=255)
plt.show()

# Essa função usa o Kmeans como um filtro de segmentação de imagem

def filtro_kmeans(img, clusters):
    vectorized = img.reshape((-1,1))
    kmeans = KMeans(n_clusters=clusters, random_state = 0, n_init=5)
    kmeans.fit(vectorized)

    centers = np.uint8(kmeans.cluster_centers_)
    segmented_data = centers[kmeans.labels_.flatten()]

    segmented_image = segmented_data.reshape((img.shape))
    return(segmented_image)

clusters = 3

img_G_segmentada = filtro_kmeans(img_G, clusters) # Tipo G
img_D_segmentada = filtro_kmeans(img_D, clusters) # Tipo D
img_F_segmentada = filtro_kmeans(img_F, clusters) # Tipo F

fig, axs = plt.subplots(1, 3, figsize=(10, 3))
im1 = axs[0].imshow(img_G_segmentada, cmap='gray', vmin=0, vmax=255)
im2 = axs[1].imshow(img_D_segmentada, cmap='gray', vmin=0, vmax=255)
im3 = axs[2].imshow(img_F_segmentada, cmap='gray', vmin=0, vmax=255)
plt.show()

"""Para mais informações de aplicação de técnicas de Agrupamento na segmentação de imagens, https://experiencor.github.io/segmentation.html"""